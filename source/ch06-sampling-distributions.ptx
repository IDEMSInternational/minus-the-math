<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch06-sampling-distributions" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Sampling Distributions</title>
  
  <introduction>
    <p>
      This is perhaps the most difficult chapter in the whole book. That is because sampling distributions are a very abstract concept that many students struggle to grasp. And yet sampling distributions are the core of how we conduct statistical inference. You may need to reread this chapter a few times before it makes much sense, but doing so will be well worth your time if you want to understand applied statistics.
    </p>
  </introduction>

  <section xml:id="sec-intro-sampling-distributions">
    <title>Introduction to Sampling Distributions</title>
    
    <p>
      Suppose you randomly sampled 10 people from the population of women in Houston, Texas, between the ages of 21 and 35 years and computed the mean height of your sample. You would not expect your sample mean to be equal to the mean of all women in Houston. It might be somewhat lower or it might be somewhat higher, but it would not equal the population mean exactly. Similarly, if you took a second sample of 10 people from the same population, you would not expect the mean of this second sample to equal the mean of the first sample.
    </p>
    
    <p>
      Recall that inferential statistics concern generalizing from a sample to a population. A critical part of inferential statistics involves determining how far sample statistics are likely to vary from each other and from the population <term>parameter</term>. (In this example, the sample statistics are the sample means and the population parameter is the population mean.) As the later portions of this chapter show, these determinations are based on sampling distributions.
    </p>
    
    <subsection xml:id="subsec-discrete-distributions">
      <title>Discrete Distributions</title>
      
      <p>
        We will illustrate the concept of sampling distributions with a simple example. <xref ref="fig-poolballs"/> shows three pool balls, each with a number on it. Two of the balls are selected randomly (with replacement) and the average of their numbers is computed.
      </p>
      
      <figure xml:id="fig-poolballs">
        <caption>The pool balls.</caption>
        <image source="Images/sampling-distributions/poolballs.jpg" width="40%"/>
      </figure>
      
      <p>
        All possible outcomes are shown below in <xref ref="tbl-twoballsoutcomes"/>.
      </p>
      
      <table xml:id="tbl-twoballsoutcomes">
        <title>All possible outcomes when two balls are sampled with replacement.</title>
        <tabular>
          <row header="yes">
            <cell>Outcome</cell>
            <cell>Ball 1</cell>
            <cell>Ball 2</cell>
            <cell>Mean</cell>
          </row>
          <row>
            <cell>1</cell>
            <cell>1</cell>
            <cell>1</cell>
            <cell>1.0</cell>
          </row>
          <row>
            <cell>2</cell>
            <cell>1</cell>
            <cell>2</cell>
            <cell>1.5</cell>
          </row>
          <row>
            <cell>3</cell>
            <cell>1</cell>
            <cell>3</cell>
            <cell>2.0</cell>
          </row>
          <row>
            <cell>4</cell>
            <cell>2</cell>
            <cell>1</cell>
            <cell>1.5</cell>
          </row>
          <row>
            <cell>5</cell>
            <cell>2</cell>
            <cell>2</cell>
            <cell>2.0</cell>
          </row>
          <row>
            <cell>6</cell>
            <cell>2</cell>
            <cell>3</cell>
            <cell>2.5</cell>
          </row>
          <row>
            <cell>7</cell>
            <cell>3</cell>
            <cell>1</cell>
            <cell>2.0</cell>
          </row>
          <row>
            <cell>8</cell>
            <cell>3</cell>
            <cell>2</cell>
            <cell>2.5</cell>
          </row>
          <row>
            <cell>9</cell>
            <cell>3</cell>
            <cell>3</cell>
            <cell>3.0</cell>
          </row>
        </tabular>
      </table>
      
      <p>
        Notice that all the means are either 1.0, 1.5, 2.0, 2.5, or 3.0. The frequencies of these means are shown in Table 6-2. The relative frequencies are equal to the frequencies divided by nine because there are nine possible outcomes.
      </p>
      
      <table xml:id="tbl-meansfreqs">
        <title>Frequencies of means for n = 2.</title>
        <tabular>
          <row header="yes">
            <cell>Mean</cell>
            <cell>Frequency</cell>
            <cell>Relative Frequency</cell>
          </row>
          <row>
            <cell>1.0</cell>
            <cell>1</cell>
            <cell>0.111</cell>
          </row>
          <row>
            <cell>1.5</cell>
            <cell>2</cell>
            <cell>0.222</cell>
          </row>
          <row>
            <cell>2.0</cell>
            <cell>3</cell>
            <cell>0.333</cell>
          </row>
          <row>
            <cell>2.5</cell>
            <cell>2</cell>
            <cell>0.222</cell>
          </row>
          <row>
            <cell>3.0</cell>
            <cell>1</cell>
            <cell>0.111</cell>
          </row>
        </tabular>
      </table>
      
      <p>
        <xref ref="fig-meansdist"/> shows a relative frequency distribution of the means based on <xref ref="tbl-meansfreqs"/>. This distribution is also a probability distribution since the Y-axis is the probability of obtaining a given mean from a sample of two balls in addition to being the relative frequency.
      </p>
      
      <figure xml:id="fig-meansdist">
        <caption>Distribution of means for n = 2.</caption>
        <image source="Images/sampling-distributions/meansdist.jpg" width="50%"/>
      </figure>
      
      <p>
        The distribution shown in <xref ref="fig-meansdist"/> is called the sampling distribution of the mean. Specifically, it is the sampling distribution of the mean for a sample size of 2 (n = 2). For this simple example, the distribution of pool balls and the sampling distribution are both discrete distributions. The pool balls have only the values 1, 2, and 3, and a sample mean can have one of only five values shown in <xref ref="tbl-meansfreqs"/>.
      </p>
      
      <p>
        There is an alternative way of conceptualizing a sampling distribution that will be useful for more complex distributions. Imagine that two balls are sampled (with replacement) and the mean of the two balls is computed and recorded. Then this process is repeated for a second sample, a third sample, and eventually thousands of samples. After thousands of samples are taken and the mean computed for each, a relative frequency distribution is drawn. The more samples, the closer the relative frequency distribution will come to the sampling distribution shown in <xref ref="fig-meansdist"/>. As the number of samples approaches infinity, the relative frequency distribution will approach the sampling distribution. This means that you can conceive of a sampling distribution as being a relative frequency distribution based on a very large number of samples. To be strictly correct, the relative frequency distribution approaches the sampling distribution as the number of samples approaches infinity.
      </p>
      
      <p>
        It is important to keep in mind that every statistic, not just the mean, has a sampling distribution. For example, <xref ref="tbl-alloutcomestwoballs"/> shows all possible outcomes for the range of two numbers (larger number minus the smaller number). <xref ref="tbl-rangesfreqs"/> shows the frequencies for each of the possible ranges and <xref ref="fig-rangesdistn2"/> shows the sampling distribution of the range.
      </p>
      
      <table xml:id="tbl-alloutcomestwoballs">
        <title>All possible outcomes when two balls are sampled with replacement.</title>
        <tabular>
          <row header="yes">
            <cell>Outcome</cell>
            <cell>Ball 1</cell>
            <cell>Ball 2</cell>
            <cell>Range</cell>
          </row>
          <row>
            <cell>1</cell>
            <cell>1</cell>
            <cell>1</cell>
            <cell>0</cell>
          </row>
          <row>
            <cell>2</cell>
            <cell>1</cell>
            <cell>2</cell>
            <cell>1</cell>
          </row>
          <row>
            <cell>3</cell>
            <cell>1</cell>
            <cell>3</cell>
            <cell>2</cell>
          </row>
          <row>
            <cell>4</cell>
            <cell>2</cell>
            <cell>1</cell>
            <cell>1</cell>
          </row>
          <row>
            <cell>5</cell>
            <cell>2</cell>
            <cell>2</cell>
            <cell>0</cell>
          </row>
          <row>
            <cell>6</cell>
            <cell>2</cell>
            <cell>3</cell>
            <cell>1</cell>
          </row>
          <row>
            <cell>7</cell>
            <cell>3</cell>
            <cell>1</cell>
            <cell>2</cell>
          </row>
          <row>
            <cell>8</cell>
            <cell>3</cell>
            <cell>2</cell>
            <cell>1</cell>
          </row>
          <row>
            <cell>9</cell>
            <cell>3</cell>
            <cell>3</cell>
            <cell>0</cell>
          </row>
        </tabular>
      </table>
      
      <table xml:id="tbl-rangesfreqs">
        <title>Distribution of ranges for n = 2.</title>
        <tabular>
          <row header="yes">
            <cell>Range</cell>
            <cell>Frequency</cell>
            <cell>Relative Frequency</cell>
          </row>
          <row>
            <cell>0</cell>
            <cell>3</cell>
            <cell>0.333</cell>
          </row>
          <row>
            <cell>1</cell>
            <cell>4</cell>
            <cell>0.444</cell>
          </row>
          <row>
            <cell>2</cell>
            <cell>2</cell>
            <cell>0.222</cell>
          </row>
        </tabular>
      </table>
      
      <figure xml:id="fig-rangesdistn2">
        <caption>Distribution of ranges for n = 2.</caption>
        <image source="Images/sampling-distributions/rangesdistn2.jpg" width="50%"/>
      </figure>
      
      <p>
        It is also important to keep in mind that there is a sampling distribution for various sample sizes. For simplicity, we have been using n = 2. The sampling distribution of the range for n = 3 is shown in <xref ref="fig-rangesdistn3"/>.
      </p>
      
      <figure xml:id="fig-rangesdistn3">
        <caption>Distribution of ranges for n = 3.</caption>
        <image source="Images/sampling-distributions/rangesdistn3.jpg" width="50%"/>
      </figure>
    </subsection>
    
    <subsection xml:id="subsec-continuous-distributions">
      <title>Continuous Distributions</title>
      
      <p>
        In the previous section, the population consisted of three pool balls. Now we will consider sampling distributions when the population distribution is continuous. What if we had a thousand pool balls with numbers ranging from 0.001 to 1.000 in equal steps? (Although this distribution is not really continuous, it is close enough to be considered continuous for practical purposes.) As before, we are interested in the distribution of means we would get if we sampled two balls and computed the mean of these two balls. In the previous example, we started by computing the mean for each of the nine possible outcomes. This would get a bit tedious for this example since there are 1,000,000 possible outcomes (1,000 for the first ball x 1,000 for the second). Therefore, it is more convenient to use our second conceptualization of sampling distributions which conceives of sampling distributions in terms of relative frequency distributions. Specifically, the relative frequency distribution that would occur if samples of two balls were repeatedly taken and the mean of each sample computed.
      </p>
      
      <p>
        When we have a truly continuous distribution, it is not only impractical but actually impossible to enumerate all possible outcomes. Moreover, in continuous distributions, the probability of obtaining any single value is zero. Therefore, these values are called probability densities rather than probabilities.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-sampling-distributions-inferential-statistics">
      <title>Sampling Distributions and Inferential Statistics</title>
      
      <p>
        As we stated in the beginning of this chapter, <term>sampling distributions</term> are important for inferential statistics. In the examples given so far, a population was specified and the sampling distribution of the mean and the range were determined. In practice, the process proceeds the other way: you collect sample data and from these data you estimate parameters of the sampling distribution. This knowledge of the sampling distribution can be very useful. For example, knowing the degree to which means from different samples would differ from each other and from the population mean would give you a sense of how close your particular sample mean is likely to be to the population mean. Fortunately, this information is directly available from a sampling distribution. The most common measure of how much sample means differ from each other is the standard deviation of the sampling distribution of the mean. This standard deviation is called the <term>standard error</term> of the mean. If all the sample means were very close to the population mean, then the standard error of the mean would be small. On the other hand, if the sample means varied considerably, then the standard error of the mean would be large.
      </p>
      
      <p>
        To be specific, assume your sample mean were 125 and you estimated that the standard error of the mean were 5 (using a method shown in a later section). If you had a normal distribution, then it would be likely that your sample mean would be within 10 units of the population mean since most of a normal distribution is within two standard deviations of the mean.
      </p>
      
      <p>
        Keep in mind that all statistics have sampling distributions, not just the mean. For example, later in this chapter we will construct confidence intervals relying on the sampling distribution for a regression slope coefficient.
      </p>
    </subsection>
  </section>
  
  <section xml:id="sec-sampling-distribution-of-the-mean">
    <title>Sampling Distribution of the Mean</title>
    
    <p>
      As we learned in the prior section, the sampling distribution of the mean refers to the probability distribution describing all possible values of the sample mean we could obtain in repeated sampling. This section goes over some important properties of the sampling distribution of the mean.
    </p>
    
    <subsection xml:id="subsec-mean-sampling-dist">
      <title>Mean</title>
      
      <p>
        The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean <m>\mu</m>, then the mean of the sampling distribution of the mean (<m>\bar{X}</m>) is also <m>\mu</m>. The symbol <m>\mu_{\bar{X}}</m> is used to refer to the mean of the sampling distribution of the mean. Therefore, the formula for the mean of the sampling distribution of the mean can be written as:
      </p>
      
      <me>
        \mu_{\bar{X}} = \mu
      </me>
    </subsection>
    
    <subsection xml:id="subsec-variance-sampling-dist">
      <title>Variance</title>
      
      <p>
        The variance of the sampling distribution of the mean is computed as follows:
      </p>
      
      <me>
        \sigma^2_{\bar{X}} = \frac{\sigma^2}{n}
      </me>
      
      <p>
        That is, the variance of the sampling distribution of the mean is the population variance divided by <m>n</m>, the sample size (the number of scores used to compute a mean). Thus, the larger the sample size, the smaller the variance of the sampling distribution of the mean.
      </p>
      
      <p>
        As noted previously, the <term>standard error</term> of the mean is the standard deviation of the sampling distribution of the mean. It is therefore the square root of the variance of the sampling distribution of the mean and can be written as:
      </p>
      
      <me>
        \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}
      </me>
      
      <p>
        The standard error is represented by a <m>\sigma</m> because it is a standard deviation. The subscript (<m>\bar{X}</m>) indicates that the standard error in question is the standard error of the (sample) mean.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-central-limit-theorem">
      <title>Central Limit Theorem</title>
      
      <p>
        The central limit theorem states that:
      </p>
      
      <blockquote>
        <p>
          Given a population with a finite mean <m>\mu</m> and a finite non-zero variance <m>\sigma^2</m>, the sampling distribution of the mean approaches a normal distribution with a mean of <m>\mu</m> and a variance of <m>\sigma^2/n</m> as <m>n</m>, the sample size, increases.
        </p>
      </blockquote>
      
      <p>
        The expressions for the mean and variance of the sampling distribution of the mean are not new or remarkable. What is remarkable is that regardless of the shape of the parent population, the sampling distribution of the mean approaches a normal distribution as n increases. <xref ref="fig-uniformsamplingdist"/> shows the results of the simulation for n = 2 and n = 10. The parent population was a uniform distribution. You can see that the distribution for n = 2 is far from a normal distribution. Nonetheless, it does show that the scores are denser in the middle than in the tails. For n = 10 the distribution is quite close to a normal distribution. Notice that the means of the two distributions are the same, but that the spread of the distribution for n = 10 is smaller.
      </p>
      
      <figure xml:id="fig-uniformsamplingdist">
        <caption>A simulation of a sampling distribution. The parent population is uniform. The blue line under <q>16</q> indicates that 16 is the mean. The red line extends from the mean plus and minus one standard deviation.</caption>
        <image source="Images/sampling-distributions/uniformsamplingdist.png" width="55%"/>
      </figure>
      
      <p>
        <xref ref="fig-nonunifirmsamplingdistsim"/> shows how closely the sampling distribution of the mean approximates a normal distribution even when the parent population is very non-normal. If you look closely you can see that the sampling distributions do have a slight positive skew. The larger the sample size, the closer the sampling distribution of the mean would be to a normal distribution.
      </p>
      
      <figure xml:id="fig-nonunifirmsamplingdistsim">
        <caption>A simulation of a sampling distribution. The parent population is very non-normal.</caption>
        <image source="Images/sampling-distributions/nonunifirmsamplingdistsim.png" width="55%"/>
      </figure>
    </subsection>
  </section>
  
  <section xml:id="sec-calculating-confidence-intervals">
    <title>Calculating Confidence Intervals</title>
    
    <subsection xml:id="subsec-confidence-intervals-mean">
      <title>Confidence Intervals for the Mean</title>
      
      <p>
        When you compute a confidence interval on the mean, you compute the mean of a sample in order to estimate the mean of the population. Clearly, if you already knew the population mean, there would be no need for a confidence interval. However, to explain how confidence intervals are constructed, we are going to work backwards and begin by assuming characteristics of the population. Then we will show how sample data can be used to construct a confidence interval.
      </p>
      
      <subsubsection xml:id="subsubsec-artificial-example-normal">
        <title>An Artificial Example: Using the Normal Distribution</title>
        
        <p>
          Assume that the weights of 10-year-old children are normally distributed with a mean of 90 and a standard deviation of 36. What is the sampling distribution of the mean for a sample size of 9? Recall from <xref ref="sec-sampling-distribution-of-the-mean"/> that the mean of the sampling distribution is <m>\mu</m> and the standard error of the mean is
        </p>
        
        <me>
          \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}
        </me>
        
        <p>
          For the present example, the sampling distribution of the mean has a mean of 90 and a standard deviation of 36/3 = 12. Note that the standard deviation of a sampling distribution is its standard error. <xref ref="fig-samplingdistn9"/> shows this distribution. The shaded area represents the middle 95% of the distribution and stretches from 66.48 to 113.52. These limits were computed by adding and subtracting 1.96 standard deviations to/from the mean of 90 as follows:
        </p>
        
        <me>
          90 - (1.96)(12) = 66.48
        </me>
        
        <me>
          90 + (1.96)(12) = 113.52
        </me>
        
        <p>
          The value of 1.96 is based on the fact that 95% of the area of a normal distribution is within 1.96 standard deviations of the mean; 12 is the standard error of the mean given our sample size of 9.
        </p>
        
        <figure xml:id="fig-samplingdistn9">
          <caption>The sampling distribution of the mean for n=9. The middle 95% of the distribution is shaded.</caption>
          <image source="Images/sampling-distributions/samplingdistn9.png" width="50%"/>
        </figure>
        
        <p>
          <xref ref="fig-samplingdistn9"/> shows that 95% of the means are no more than 23.52 units (1.96 standard deviations) from the mean of 90. Now consider the probability that a sample mean computed in a random sample is within 23.52 units of the population mean of 90. Since 95% of the distribution is within 23.52 of 90, the probability that the mean from any given sample will be within 23.52 of 90 is 0.95. This means that if we repeatedly compute the mean (<m>\bar{X}</m>) from a sample (drawing a new random sample each time), and create an interval ranging from <m>\bar{X} - 23.52</m> to <m>\bar{X} + 23.52</m>, this interval will contain the population mean 95% of the time. In general, you compute the 95% confidence interval for the mean with the following formula:
        </p>
        
        <md>
          <mrow>\text{Lower limit} = \bar{X} - Z_{.95}\times\sigma_{\bar{X}}</mrow>
          <mrow>\text{Upper limit} = \bar{X} + Z_{.95}\times\sigma_{\bar{X}}</mrow>
        </md>
        
        <p>
          where <m>Z_{.95}</m> is the number of standard deviations extending from the mean of a normal distribution required to contain 0.95 of the area (always equal to 1.96) and <m>\sigma_{\bar{X}}</m> is the standard error of the mean.
        </p>
        
        <p>
          If you look closely at this formula for a confidence interval, you will notice that you need to know the standard deviation (<m>\sigma</m>) in order to estimate the mean. This may sound unrealistic, and it is. However, computing a confidence interval when <m>\sigma</m> is known is easier than when <m>\sigma</m> has to be estimated, and serves a pedagogical purpose. Later in this section we will show how to compute a confidence interval for the mean when <m>\sigma</m> has to be estimated.
        </p>
        
        <p>
          Suppose the following five numbers were sampled from a normal distribution with a standard deviation of 2.5: 2, 3, 5, 6, and 9. To compute the 95% confidence interval, start by computing the mean and standard error:
        </p>
        
        <me>
          \bar{X} = (2 + 3 + 5 + 6 + 9)/5 = 5.
        </me>
        
        <me>
          \sigma_{\bar{X}}=\frac{2.5}{\sqrt{5}}=1.118.
        </me>
        
        <p>
          Z<m>_{.95}</m> can be found using the normal distribution calculator and specifying that the shaded area is 0.95 and indicating that you want the area to be between the cutoff points. As shown in <xref ref="fig-normaldistcalc"/>, the value is 1.96. If you had wanted to compute the 99% confidence interval, you would have set the shaded area to 0.99 and the result would have been 2.58.
        </p>
        
        <figure xml:id="fig-normaldistcalc">
          <caption>95% of the area is between -1.96 and 1.96.</caption>
          <image source="Images/sampling-distributions/normaldistcalc.png" width="45%"/>
        </figure>
        
        <p>
          The confidence interval can then be computed as follows:
        </p>
        
        <md>
          <mrow>\text{Lower limit} = 5 - (1.96)(1.118)= 2.81</mrow>
          <mrow>\text{Upper limit} = 5 + (1.96)(1.118)= 7.19</mrow>
        </md>
      </subsubsection>
      
      <subsubsection xml:id="subsubsec-realistic-case-t-distribution">
        <title>The Realistic Case: Using the T Distribution</title>
        
        <p>
          You should use the t distribution rather than the normal distribution when the variance is not known and has to be estimated from sample data. You will learn more about the t distribution in the next section. When the sample size is large, say 100 or above, the t distribution is very similar to the standard normal distribution. However, with smaller sample sizes, the t distribution has relatively more scores in its tails than does the normal distribution. As a result, you have to extend farther from the mean to contain a given proportion of the area. Recall that with a normal distribution, 95% of the distribution is within 1.96 standard deviations of the mean. Using the t distribution, if you have a sample size of only 5, 95% of the area is within 2.78 standard deviations of the mean. Therefore, the standard error of the mean would be multiplied by 2.78 rather than 1.96.
        </p>
        
        <p>
          The values of t to be used in a confidence interval can be looked up in a table of the t distribution, a small version of which is provided in the following section. You can also use an <q>inverse t distribution</q> calculator to find the t values to use in confidence intervals. With either approach, the t values will vary depending upon the degrees of freedom (df). For confidence intervals on the mean, df is equal to n - 1, where n is the sample size.
        </p>
        
        <p>
          Assume that the following five numbers are sampled from a normal distribution: 2, 3, 5, 6, and 9 and that the standard deviation is not known. The first steps are to compute the sample mean and variance:
        </p>
        
        <me>
          \bar{X} = 5
        </me>
        
        <me>
          s^2 = 7.5
        </me>
        
        <p>
          The next step is to estimate the standard error of the mean. If we knew the population variance, we could use the following formula:
        </p>
        
        <me>
          \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}
        </me>
        
        <p>
          Instead we compute an estimate of the standard error (<m>s_{\bar{X}}</m>). Note that since we previously computed the sample variance (<m>s^2 = 7.5</m>), we can take the square root of this to obtain the sample standard deviation (<m>s</m>):
        </p>
        
        <me>
          s_{\bar{X}} = \frac{s}{\sqrt{n}} = \frac{\sqrt{7.5}}{\sqrt{5}} = 1.225
        </me>
        
        <p>
          The next step is to find the value of t. As shown in <xref ref="tbl-abbrevttable"/> of the following section, the value for the 95% interval for df = n - 1 = 4 is 2.776. The confidence interval is then computed just as it is when <m>\sigma_{\bar{X}}</m>. The only differences are that <m>s_{\bar{X}}</m> and <m>t</m> rather than <m>\sigma_{\bar{X}}</m> and <m>Z</m> are used.
        </p>
        
        <md>
          <mrow>\text{Lower limit} = 5 - (2.776)(1.225) = 1.60</mrow>
          <mrow>\text{Upper limit} = 5 + (2.776)(1.225) = 8.40</mrow>
        </md>
        
        <p>
          More generally, the formula for the 95% confidence interval on the mean is:
        </p>
        
        <md>
          <mrow>\text{Lower limit} = \bar{X} - (t_{CL})(s_{\bar{X}})</mrow>
          <mrow>\text{Upper limit} = \bar{X} + (t_{CL})(s_{\bar{X}})</mrow>
        </md>
        
        <p>
          where <m>\bar{X}</m> is the sample mean, <m>t_{CL}</m> is the t for the confidence level desired (0.95 in the above example), and <m>s_{\bar{X}}</m> is the estimated standard error of the mean.
        </p>
        
        <p>
          We will finish our discussion of confidence intervals for the mean with an analysis of the Stroop Data. Specifically, we will compute a confidence interval on the mean difference score. As mentioned in <xref ref="subsec-box-plots"/>, the study involved 47 subjects naming the color of ink that words were written in. An additional detail that is now relevant to us is that subjects completed similar naming tasks multiple times under different conditions. In the <q>interference</q> condition, the names conflicted so that, for example, they would name the ink color of the word <q>blue</q> written in red ink. The correct response is to say <q>red</q> and ignore the fact that the word is <q>blue.</q> In a second condition, subjects named the ink color of colored rectangles.
        </p>
        
        <table xml:id="tbl-responsetime">
          <title>Response times in seconds for 10 subjects.</title>
          <tabular>
            <row header="yes">
              <cell>Naming Colored Rectangle</cell>
              <cell>Interference</cell>
              <cell>Difference</cell>
            </row>
            <row>
              <cell>17</cell>
              <cell>38</cell>
              <cell>21</cell>
            </row>
            <row>
              <cell>15</cell>
              <cell>58</cell>
              <cell>43</cell>
            </row>
            <row>
              <cell>18</cell>
              <cell>35</cell>
              <cell>17</cell>
            </row>
            <row>
              <cell>20</cell>
              <cell>39</cell>
              <cell>19</cell>
            </row>
            <row>
              <cell>18</cell>
              <cell>33</cell>
              <cell>15</cell>
            </row>
            <row>
              <cell>20</cell>
              <cell>32</cell>
              <cell>12</cell>
            </row>
            <row>
              <cell>20</cell>
              <cell>45</cell>
              <cell>25</cell>
            </row>
            <row>
              <cell>19</cell>
              <cell>52</cell>
              <cell>33</cell>
            </row>
            <row>
              <cell>17</cell>
              <cell>31</cell>
              <cell>14</cell>
            </row>
            <row>
              <cell>21</cell>
              <cell>29</cell>
              <cell>8</cell>
            </row>
          </tabular>
        </table>
        
        <p>
          <xref ref="tbl-responsetime"/> shows the time difference between the interference and color-naming conditions for 10 of the 47 subjects. The mean time difference for all 47 subjects is 16.362 seconds and the standard deviation is 7.470 seconds. The standard error of the mean is 1.090. A t table shows the critical value of t for 47 - 1 = 46 degrees of freedom is 2.013 (for a 95% confidence interval). Therefore the confidence interval is computed as follows:
        </p>
        
        <md>
          <mrow>\text{Lower limit} = 16.362 - (2.013)(1.090) = 14.17</mrow>
          <mrow>\text{Upper limit} = 16.362 + (2.013)(1.090) = 18.56</mrow>
        </md>
        
        <p>
          Therefore, the interference effect (difference) for the whole population is likely to be between 14.17 and 18.56 seconds.
        </p>
      </subsubsection>
    </subsection>
    
    <subsection xml:id="subsec-more-about-t-distribution">
      <title>More about the T Distribution</title>
      
      <p>
        In the introduction to normal distributions it was shown that 95% of the area of a normal distribution is within 1.96 standard deviations of the mean. Therefore, if you randomly sampled a value from a normal distribution with a mean of 100, the probability it would be within 1.96<m>\sigma</m> of 100 is 0.95. Similarly, if you sample n values from the population, the probability that the sample mean (<m>\bar{X}</m>) will be within 1.96 <m>\sigma_{\bar{X}}</m> of 100 is 0.95.
      </p>
      
      <p>
        Now consider the case in which you have a normal distribution but you do not know the standard deviation. You sample <m>n</m> values and compute the sample mean (<m>\bar{X}</m>) and estimate the standard error of the mean (<m>\sigma_{\bar{X}}</m>) with <m>s_{\bar{X}}</m>. What is the probability that <m>\bar{X}</m> will be within 1.96 <m>s_{\bar{X}}</m> of the population mean (<m>\mu</m>)? This is a difficult problem because there are two ways in which <m>\bar{X}</m> could be more than 1.96 <m>s_{\bar{X}}</m> from <m>\mu</m>: (1) <m>\bar{X}</m> could, by chance, be either very high or very low and (2) <m>s_{\bar{X}}</m> could, by chance, be very low. Intuitively, it makes sense that the probability of being within 1.96 standard errors of the mean should be smaller than in the case when the standard deviation is known (and cannot be underestimated). But exactly how much smaller? Fortunately, the way to work out this type of problem was solved in the early 20th century by W. S. Gosset who determined the distribution of a mean divided by an estimate of its standard error. This distribution is called the <em>Student's t distribution</em> or sometimes just the t distribution. Gosset worked out the t distribution and associated statistical tests while working for a brewery in Ireland. Because of a contractual agreement with the brewery, he published the article under the pseudonym <q>Student.</q> That is why the t test is called the <q>Student's t test.</q>
      </p>
      
      <p>
        The <term>t distribution</term> is very similar to the normal distribution when the estimate of variance is based on a large sample, but the t distribution has relatively more scores in its tails when there is a small sample. When working with the t distribution, sample size is expressed in what are called <term>degrees of freedom</term>. Degrees of freedom indicate the number of independent pieces of information on which an estimate is based; a more complete discussion of the concept is provided in Appendix I at the end of this chapter. As we noted in the prior section, when we are estimating the standard error for a sample mean, the degrees of freedom is simply equal to the sample size minus one (n-1).
      </p>
      
      <p>
        <xref ref="fig-tdiststandardsd"/> shows t distributions with 2, 4, and 10 degrees of freedom and the standard normal distribution. Notice that the normal distribution has relatively more scores in the center of the distribution and the t distribution has relatively more in the tails. The t distribution approaches the normal distribution as the degrees of freedom increase.
      </p>
      
      <figure xml:id="fig-tdiststandardsd">
        <caption>A comparison of t distributions with 2, 4, and 10 df and the standard normal distribution. The distribution with the lowest peak is the 2 df distribution, the next lowest is 4 df, the lowest after that is 10 df, and the highest is the standard normal distribution.</caption>
        <image source="Images/sampling-distributions/tdiststandardsd.jpg" width="60%"/>
      </figure>
      
      <p>
        Since the t distribution has more area in the tails, the percentage of the distribution within 1.96 standard deviations of the mean is less than the 95% for the normal distribution. <xref ref="tbl-abbrevttable"/> shows the number of standard deviations from the mean required to contain 95% and 99% of the area of the t distribution for various degrees of freedom. These are the values of t that you use in a confidence interval. The corresponding values for the normal distribution are 1.96 and 2.58 respectively. Notice that with few degrees of freedom, the values of t are much higher than the corresponding values for a normal distribution and that the difference decreases as the degrees of freedom increase. , since the critical t value for probability 0.95 got closer and closer to 1.96 (the critical value for 0.95 in the normal distribution) as the degrees of freedom incrased. The values shown in Table 6-7 can be obtained from statistical software or an online calculator.
      </p>
      
      <table xml:id="tbl-abbrevttable">
        <title>Abbreviated t table.</title>
        <tabular>
          <row header="yes">
            <cell>df</cell>
            <cell>0.95</cell>
            <cell>0.99</cell>
          </row>
          <row>
            <cell>2</cell>
            <cell>4.303</cell>
            <cell>9.925</cell>
          </row>
          <row>
            <cell>3</cell>
            <cell>3.182</cell>
            <cell>5.841</cell>
          </row>
          <row>
            <cell>4</cell>
            <cell>2.776</cell>
            <cell>4.604</cell>
          </row>
          <row>
            <cell>5</cell>
            <cell>2.571</cell>
            <cell>4.032</cell>
          </row>
          <row>
            <cell>8</cell>
            <cell>2.306</cell>
            <cell>3.355</cell>
          </row>
          <row>
            <cell>10</cell>
            <cell>2.228</cell>
            <cell>3.169</cell>
          </row>
          <row>
            <cell>20</cell>
            <cell>2.086</cell>
            <cell>2.845</cell>
          </row>
          <row>
            <cell>50</cell>
            <cell>2.009</cell>
            <cell>2.678</cell>
          </row>
          <row>
            <cell>100</cell>
            <cell>1.984</cell>
            <cell>2.626</cell>
          </row>
        </tabular>
      </table>
      
      <p>
        Returning to the problem posed at the beginning of this section, suppose you sampled 9 values from a normal population and estimated the standard error of the mean (<m>\sigma_{\bar{X}}</m>) with <m>s_{\bar{X}}</m>. What is the probability that <m>\bar{X}</m> would be within 1.96<m>s_{\bar{X}}</m> of <m>\mu</m>? Since the sample size is 9, there are n - 1 = 8 df. From <xref ref="tbl-abbrevttable"/>, you can see that with 8 df the probability is 0.95 that the mean will be within 2.306 <m>s_{\bar{X}}</m> of <m>\mu</m>. The probability that it will be within 1.96 <m>s_{\bar{X}}</m> of <m>\mu</m> is therefore lower than 0.95.
      </p>
      
      <p>
        As shown in <xref ref="fig-tdist8df"/>, a t distribution calculator can be used to find that 0.086 of the area of a t distribution is more than 1.96 standard deviations from the mean, so the probability that <m>\bar{X}</m> would be less than 1.96<m>s_{\bar{X}}</m> from <m>\mu</m> is 1 - 0.086 = 0.914.
      </p>
      
      <figure xml:id="fig-tdist8df">
        <caption>Area more than 1.96 standard deviations from the mean in a t distribution with 8 df. Note that the two-tailed button is selected so that the area in both tails will be included.</caption>
        <image source="Images/sampling-distributions/tdist8df.png" width="45%"/>
      </figure>
      
      <p>
        As expected, this probability is less than 0.95 that would have been obtained if <m>\sigma_{\bar{X}}</m> had been known instead of estimated.
      </p>
    </subsection>
    
    <subsection xml:id="subsec-confidence-intervals-regression-slope">
      <title>Confidence Intervals for a Regression Slope Coefficient</title>
      
      <p>
        The method for computing a confidence interval for the population slope in a simple linear regression is very similar to methods for computing other confidence intervals. For the 95% confidence interval, the formula is:
      </p>
      
      <md>
        <mrow>\text{Lower limit} = \hat{\beta} - (t_{.95})(s_{\beta})</mrow>
        <mrow>\text{Upper limit} = \hat{\beta} + (t_{.95})(s_{\beta})</mrow>
      </md>
      
      <p>
        where <m>\hat{\beta}</m> is the slope coefficient estimate, <m>t_{.95}</m> is the value of t for 95% (2-tailed) confidence, and <m>s_{\beta}</m> is the standard error for the slope estimate. As before, the t value can be found from a table or an inverse t distribution calculator based on the degrees of freedom.
      </p>
      
      <p>
        We illustrate generating a confidence interval using the same data as in the example from <xref ref="sec-introduction-linear-regression"/>, depicted again here as <xref ref="fig-reg-example-cis"/>.
      </p>
      
      <figure xml:id="fig-reg-example-cis">
        <caption>A scatter plot of the example data. The black line consists of the predictions, the points are the actual data, and the vertical lines between the points and the black line represent errors of prediction.</caption>
        <image source="Images/correlations/splotexampledatadetail.jpg" width="50%"/>
      </figure>
      
      <p>
        When conducting statistical inference for linear regression coefficients, the degrees of freedom is equal to the number of observations minus the number of coefficients being estimated (usually one for the intercept plus one for each independent variable). In the case of simple regression, there is just one independent variable plus a y-intercept, so the number of degrees of freedom is:
      </p>
      
      <me>
        df = n-2
      </me>
      
      <p>
        where <m>n</m> is the number of pairs of scores (number of observations in the sample).
      </p>
      
      <p>
        As we saw in <xref ref="sec-introduction-linear-regression"/>, the estimated regression slope coefficient is 0.425 with this data. An <m>n</m> of 5 yields 3 degrees of freedom (5 - 2 = 3), which means the critical t value (at 95% confidence) is 3.182. Finally, the estimated standard error for this slope coefficient (the calculative of which is shown in this chapter's Appendix II) is 0.305 with this data.
      </p>
      
      <p>
        Applying these formulas we obtain a confidence interval with the following lower and upper limits:
      </p>
      
      <md>
        <mrow>\text{Lower limit} = 0.425 - (3.182)(0.305) = -0.55</mrow>
        <mrow>\text{Upper limit} = 0.425 + (3.182)(0.305) = 1.40</mrow>
      </md>
      
      <subsubsection xml:id="subsubsec-assumptions-regression-inference">
        <title>Assumptions for Statistical Inference from Regression</title>
        
        <p>
          Although no assumptions were needed to determine the best-fitting straight line, assumptions are made in the calculation of inferential statistics. Naturally, these assumptions refer to the population, not the sample.
        </p>
        
        <ol>
          <li>
            <p>
              Linearity: The relationship between the two variables is linear.
            </p>
          </li>
          
          <li>
            <p>
              Homoscedasticity: The variance around the regression line is the same for all values of X. A clear violation of this assumption is shown in <xref ref="fig-homoscedasticity"/>. Notice that the predictions for students with high high-school GPAs are very good, whereas the predictions for students with low high-school GPAs are not very good. In other words, the points for students with high high-school GPAs are close to the regression line, whereas the points for low high-school GPA students are not.
            </p>
          </li>
          
          <li>
            <p>
              The errors of prediction are distributed normally. This means that the deviations from the regression line are normally distributed. It does not mean that X or Y is normally distributed.
            </p>
          </li>
        </ol>
        
        <figure xml:id="fig-homoscedasticity">
          <caption>University GPA as a function of High School GPA.</caption>
          <image source="Images/sampling-distributions/homoscedasticty.jpg" width="50%"/>
        </figure>
        
        <p>
          We will return to the topic of model assumptions and the consequences of violating them in <xref ref="ch11-models-uncertainty"/>.
        </p>
      </subsubsection>
    </subsection>
  </section>
  
  <section xml:id="sec-appendix-degrees-freedom">
    <title>Appendix I: Degrees of Freedom</title>
    
    <p>
      Some estimates are based on more information than others. For example, an estimate of the variance based on a sample size of 100 is based on more information than an estimate of the variance based on a sample size of 5. The <term>degrees of freedom (df)</term> of an estimate is the number of independent pieces of information on which the estimate is based.
    </p>
    
    <p>
      As an example, let's say that we know that the mean height of Martians is 6 and wish to estimate the variance of their heights. We randomly sample one Martian and find that its height is 8. Recall that the variance is defined as the mean squared deviation of the values from their population mean. We can compute the squared deviation of our value of 8 from the population mean of 6 to find a single squared deviation from the mean. This single squared deviation from the mean, (8-6)<m>^2</m> = 4, is an estimate of the mean squared deviation for all Martians. Therefore, based on this sample of one, we would estimate that the population variance is 4. This estimate is based on a single piece of information and therefore has 1 df. If we sampled another Martian and obtained a height of 5, then we could compute a second estimate of the variance, (5-6)<m>^2</m> = 1. We could then average our two estimates (4 and 1) to obtain an estimate of 2.5. Since this estimate is based on two independent pieces of information, it has two degrees of freedom. The two estimates are independent because they are based on two independently and randomly selected Martians. The estimates would not be independent if after sampling one Martian, we decided to choose its brother as our second Martian.
    </p>
    
    <p>
      As you are probably thinking, it is pretty rare that we know the population mean when we are estimating the variance. Instead, we have to first estimate the population mean (<m>\mu</m>) with the sample mean (<m>\bar{X}</m>). The process of estimating the mean affects our degrees of freedom as shown below.
    </p>
    
    <p>
      Returning to our problem of estimating the variance in Martian heights, let's assume we do not know the population mean and therefore we have to estimate it from the sample. We have sampled two Martians and found that their heights are 8 and 5. Therefore <m>\bar{X}</m>, our estimate of the population mean, is
    </p>
    
    <me>
      \bar{X} = (8+5)/2 = 6.5.
    </me>
    
    <p>
      We can now compute two estimates of variance:
    </p>
    
    <me>
      \text{Estimate 1} = (8-6.5)^2 = 2.25
    </me>
    
    <me>
      \text{Estimate 2} = (5-6.5)^2 = 2.25
    </me>
    
    <p>
      Now for the key question: Are these two estimates independent? The answer is no because each height contributed to the calculation of <m>\bar{X}</m>. Since the first Martian's height of 8 influenced <m>\bar{X}</m>, it also influenced Estimate 2. If the first height had been, for example, 10, then <m>\bar{X}</m> would have been 7.5 and Estimate 2 would have been (5-7.5)<m>^2</m> = 6.25 instead of 2.25. The important point is that the two estimates are not independent and therefore we do not have two degrees of freedom. Another way to think about the non-independence is to consider that if you knew the mean and one of the scores, you would know the other score. For example, if one score is 5 and the mean is 6.5, you can compute that the total of the two scores is 13 and therefore that the other score must be 13-5 = 8.
    </p>
    
    <p>
      In general, the degrees of freedom for an estimate is equal to the number of values minus the number of parameters estimated en route to the estimate in question. In the Martians example, there are two values (8 and 5) and we had to estimate one parameter (<m>\mu</m>) on the way to estimating the parameter of interest (<m>\sigma^2</m>). Therefore, the estimate of variance has 2 - 1 = 1 degree of freedom. If we had sampled 12 Martians, then our estimate of variance would have had 11 degrees of freedom. Therefore, the degrees of freedom of an estimate of variance is equal to n - 1, where n is the number of observations.
    </p>
    
    <p>
      Recall from <xref ref="subsec-variance"/> that the formula for estimating the variance in a sample is:
    </p>
    
    <me>
      s^2=\frac{\sum{(X-\bar{X})^2}}{n-1}
    </me>
    
    <p>
      The denominator of this formula is the degrees of freedom.
    </p>
    
    <p>
      So far, we've mainly seen examples where the degrees of freedom is equal to n - 1. But as we saw in <xref ref="subsec-confidence-intervals-regression-slope"/>, the degrees of freedom can also be equal to other values such as n - 2. In the case of constructing a confidence interval for a coefficient from a multiple regression with four independent variables, the degrees of freedom will generally be equal to n - 5. The exact formula for degrees of freedom depends on what we are estimating.
    </p>
  </section>
  
  <section xml:id="sec-appendix-standard-error-regression-slope">
    <title>Appendix II: Estimating the Standard Error of a Regression Slope</title>
    
    <p>
      This appendix shows how to compute the estimated standard error for the slope of a simple linear regression. The estimated standard error of <m>\beta</m> is computed using the following formula:
    </p>
    
    <me>
      s_{\beta} = \frac{s_{est}}{\sqrt{SSX}}
    </me>
    
    <p>
      where <m>s_{\beta}</m> is the estimated standard error of <m>\beta</m>, <m>s_{est}</m> is the standard error of the estimate, and <m>SSX</m> is the sum of squared deviations of <m>X</m> from the mean of <m>X</m>. <m>SSX</m> is calculated as:
    </p>
    
    <me>
      SSX = \sum{(X-\bar{X})^2}
    </me>
    
    <p>
      where <m>\bar{X}</m> is the mean of <m>X</m>. The standard error of the estimate can be calculated as:
    </p>
    
    <me>
      s_{est} = \sqrt{\frac{(1-r^2)SSY}{n-2}}
    </me>
    
    <p>
      where <m>r</m> is the correlation between <m>X</m> and <m>Y</m>, and <m>SSY</m> is the sum of squared deviations of <m>Y</m> from the mean of <m>Y</m>.
    </p>
    
    <p>
      These formulas are illustrated with the data shown in <xref ref="tbl-example-calc-st-err"/>. These data are reproduced from <xref ref="sec-introduction-linear-regression"/>. The column X has the values of the independent variable and the column Y has the values of the dependent variable. The third column, x, contains the differences between the values of column X and the mean of X. The fourth column, x<m>^2</m>, is the square of the x column. The fifth column, y, contains the differences between the values of column Y and the mean of Y. The last column, y<m>^2</m>, is simply square of the y column.
    </p>
    
    <table xml:id="tbl-example-calc-st-err">
      <title>Example data.</title>
      <tabular>
        <row header="yes">
          <cell>X</cell>
          <cell>Y</cell>
          <cell>x</cell>
          <cell>x<m>^2</m></cell>
          <cell>y</cell>
          <cell>y<m>^2</m></cell>
        </row>
        <row>
          <cell>1.00</cell>
          <cell>1.00</cell>
          <cell>-2.00</cell>
          <cell>4</cell>
          <cell>-1.06</cell>
          <cell>1.1236</cell>
        </row>
        <row>
          <cell>2.00</cell>
          <cell>2.00</cell>
          <cell>-1.00</cell>
          <cell>1</cell>
          <cell>-0.06</cell>
          <cell>0.0036</cell>
        </row>
        <row>
          <cell>3.00</cell>
          <cell>1.30</cell>
          <cell>0.00</cell>
          <cell>0</cell>
          <cell>-0.76</cell>
          <cell>0.5776</cell>
        </row>
        <row>
          <cell>4.00</cell>
          <cell>3.75</cell>
          <cell>1.00</cell>
          <cell>1</cell>
          <cell>1.69</cell>
          <cell>2.8561</cell>
        </row>
        <row>
          <cell>5.00</cell>
          <cell>2.25</cell>
          <cell>2.00</cell>
          <cell>4</cell>
          <cell>0.19</cell>
          <cell>0.0361</cell>
        </row>
        <row>
          <cell>Sum</cell>
          <cell>15.00</cell>
          <cell>10.30</cell>
          <cell>0.00</cell>
          <cell>10.00</cell>
          <cell>0.00</cell>
          <cell>4.5970</cell>
        </row>
      </tabular>
    </table>
    
    <p>
      <m>SSY</m> is the sum of squared deviations from the mean of Y. It is, therefore, equal to the sum of the y<m>^2</m> column and is equal to 4.597. The correlation (<m>r</m>) between <m>X</m> and <m>Y</m> is 0.6268, and there are 5 observations (n=5). Thus, the standard error of the estimate is:
    </p>
    
    <me>
      s_{est} = \sqrt{\frac{(1-(0.6268)^2)(4.597)}{5-2}} = \sqrt{\frac{2.791}{3}} = 0.964
    </me>
    
    <p>
      <m>SSX</m> can be found as the sum of the x<m>^2</m> column and is equal to 10.
    </p>
    
    <p>
      We now have all the information to compute the standard error of <m>\beta</m>:
    </p>
    
    <me>
      s_{\beta} = \frac{0.964}{\sqrt{10}} = 0.305
    </me>
  </section>

</chapter>
